{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arise/anaconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "from hdbscan.flat import (HDBSCAN_flat,\n",
    "\t\t\t\t\t\t  approximate_predict_flat,\n",
    "\t\t\t\t\t\t  membership_vector_flat,\n",
    "\t\t\t\t\t\t  all_points_membership_vectors_flat)\n",
    "# PLOT CLUSTERS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import text_process as tp\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, y):\n",
    "\n",
    "\ttexts = []\n",
    "\tlabels = []\n",
    "\n",
    "\twith open(path, encoding='utf-8') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\ttexts.append(line)\n",
    "\t\t\tlabels.append(y)\n",
    "\n",
    "\treturn texts, labels\n",
    "\n",
    "def load_data():\n",
    "\t# load data\n",
    "\tpaths = ['../data/train.negative.csv',\n",
    "\t\t\t\t'../data/train.non-negative.csv',\n",
    "\t\t\t\t'../data/test.negative.csv',\n",
    "\t\t\t\t'../data/test.non-negative.csv']\n",
    "\n",
    "\t# get data as list of dict with text and label\n",
    "\ttrain_neg_x_ls, train_neg_y_ls = get_data(paths[0], 1)\n",
    "\ttrain_non_x_ls, train_non_y_ls = get_data(paths[1], 0)\n",
    "\ttest_neg_x_ls, test_neg_y_ls = get_data(paths[2], 1)\n",
    "\ttest_non_x_ls, test_non_y_ls = get_data(paths[3], 0)\n",
    "\n",
    "\ttrain_data = train_neg_x_ls + train_non_x_ls\n",
    "\t# test_data = test_neg_x_ls + test_non_x_ls\n",
    "\n",
    "\tprint('train data length: ', len(train_data))\n",
    "\t# 14643\n",
    "\n",
    "\n",
    "\ttrain_data = [tp.remove_punctuation(sentence) for sentence in train_data]\n",
    "\ttrain_data = [sentence.lower() for sentence in train_data]\n",
    "\ttrain_data = [sentence.strip().split() for sentence in train_data]\n",
    "\n",
    "\ttrain_data = [sentence for sentence in train_data if len(sentence) >= 5]\n",
    "\n",
    "\ttrain_data = [' '.join(sentence) for sentence in train_data]\n",
    "\n",
    "\tnlp = spacy.load('en_core_web_sm')\n",
    "\ttrain_data = [[token.lemma_ for token in nlp(sentence)] for sentence in train_data]\n",
    "\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttrain_data = [ [word for word in sentence if not word in stop_words and word != ' '] for sentence in train_data]\n",
    "\n",
    "\ttrain_data = [' '.join(sentence) for sentence in train_data]\n",
    "\n",
    "\tprint('train data length (after preprocessing): ', len(train_data))\n",
    "\n",
    "\treturn train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(train_data):\n",
    "\t# BRING MODEL & ENCODE DATA to EMBEDDING VALUE\n",
    "\t'''\n",
    "\tdistilbert gives good balance between speed and performance\n",
    "\tsupports multi-lingual models\n",
    "\t'''\n",
    "\tfrom sentence_transformers import SentenceTransformer\n",
    "\tmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\tembeddings = model.encode(train_data, show_progress_bar=True)\n",
    "\n",
    "\tprint('embeddings: ', embeddings.shape)\n",
    "\t# (14643, 768)\n",
    "\n",
    "\treturn embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "\n",
    "\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names_out()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(embeddings, n_neighbors, n_components=5, min_dist=0.0, min_cluster_size=5):\n",
    "\t# REDUCE EMBEDDING DIMENSIONS using UMAP\n",
    "\tumap_embeddings = umap.UMAP(n_neighbors=n_neighbors,\n",
    "\t\t\t\t\t\t\t\tn_components=n_components,\n",
    "\t\t\t\t\t\t\t\tmin_dist=min_dist,\n",
    "\t\t\t\t\t\t\t\tmetric='cosine').fit_transform(embeddings)\n",
    "\tprint('\\ndone with reducing dimension')\n",
    "\n",
    "\n",
    "\t# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\t# CLUSTER EMBEDDINGS using HDBSCAN\n",
    "\t'''\n",
    "\tdoes not force data points meaning knows outliers\n",
    "\t'''\n",
    "\tcluster = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "\t\t\t\t\t\t\t\tmetric='euclidean',\n",
    "\t\t\t\t\t\t\t\tcluster_selection_method='eom').fit(umap_embeddings)\n",
    "\t'''\n",
    "\tcluster = HDBSCAN_flat(umap_embeddings,\n",
    "\t\t\t\t\t\t\tcluster_selection_method='eom',\n",
    "\t\t\t\t\t\t\tn_clusters=20,\n",
    "\t\t\t\t\t\t\tmin_cluster_size=min_cluster_size)\n",
    "\t'''\t\n",
    "\n",
    "\tprint('\\ndone with clustering')\n",
    "\n",
    "\n",
    "\t# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\t# Prepare data\n",
    "\tumap_data = umap.UMAP(n_neighbors=n_neighbors,\n",
    "\t\t\t\t\t\t\tn_components=2,\n",
    "\t\t\t\t\t\t\tmin_dist=min_dist,\n",
    "\t\t\t\t\t\t\tmetric='cosine').fit_transform(embeddings)\n",
    "\tresult = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "\tresult['labels'] = cluster.labels_\n",
    "\n",
    "\n",
    "\t# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\t# Visualize clusters\n",
    "\tfig, ax = plt.subplots(figsize=(20, 10))\n",
    "\toutliers = result.loc[result.labels == -1, :]\n",
    "\tclustered = result.loc[result.labels != -1, :]\n",
    "\tplt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "\tplt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "\tplt.colorbar()\n",
    "\tplt.show()\n",
    "\n",
    "\tprint('\\ndone with plot')\n",
    "\n",
    "\n",
    "\t# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\t#c-TF-IDF\n",
    "\tdocs_df = pd.DataFrame(train_data, columns=[\"Doc\"])\n",
    "\tdocs_df['Topic'] = cluster.labels_\n",
    "\tdocs_df['Doc_ID'] = range(len(docs_df))\n",
    "\tdocs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
    "\n",
    "\n",
    "\t  \n",
    "\ttf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(train_data))\n",
    "\n",
    "\n",
    "\n",
    "\ttop_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "\ttopic_sizes = extract_topic_sizes(docs_df);\n",
    "\n",
    "\tprint(topic_sizes.head(10))\n",
    "\tprint(len(topic_sizes))\n",
    "\n",
    "\n",
    "\tcnt = 0\n",
    "\tfor i, row in topic_sizes.iterrows():\n",
    "\t\ttopic_num = row['Topic']\n",
    "\t\ttopic_size = row['Size']\n",
    "\n",
    "\t\tif topic_num == -1: continue\n",
    "\n",
    "\t\tprint(topic_size, top_n_words[topic_num][:10])\n",
    "\n",
    "\t\tif cnt == 4: break\n",
    "\t\tcnt += 1\n",
    "\t\n",
    "\treturn docs_per_topic, top_n_words, topic_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_data = load_data()\\nembeddings = embed(train_data)\\n\\n\\n# MAIN\\n# def plot(embeddings, n_neighbors, n_components=5, min_dist=0.0, min_cluster_size=5):\\nchng_neigh = [[2, 5, 0.0, 15],\\n\\t\\t\\t  [5, 5, 0.0, 15],\\n\\t\\t\\t  [10, 5, 0.0, 15],\\n\\t\\t\\t  [20, 5, 0.0, 15],\\n\\t\\t\\t  [50, 5, 0.0, 15],\\n\\t\\t\\t  [100, 5, 0.0, 15]]\\n\\n\\nfor i in range(len(chng_neigh)):\\n\\tplot(embeddings, chng_neigh[i][0], chng_neigh[i][1], chng_neigh[i][2], chng_neigh[i][3])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_data = load_data()\n",
    "embeddings = embed(train_data)\n",
    "\n",
    "\n",
    "# MAIN\n",
    "# def plot(embeddings, n_neighbors, n_components=5, min_dist=0.0, min_cluster_size=5):\n",
    "chng_neigh = [[2, 5, 0.0, 15],\n",
    "\t\t\t  [5, 5, 0.0, 15],\n",
    "\t\t\t  [10, 5, 0.0, 15],\n",
    "\t\t\t  [20, 5, 0.0, 15],\n",
    "\t\t\t  [50, 5, 0.0, 15],\n",
    "\t\t\t  [100, 5, 0.0, 15]]\n",
    "\n",
    "\n",
    "for i in range(len(chng_neigh)):\n",
    "\tplot(embeddings, chng_neigh[i][0], chng_neigh[i][1], chng_neigh[i][2], chng_neigh[i][3])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data length:  14643\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[39m=\u001b[39m load_data()\n\u001b[1;32m      2\u001b[0m embeddings \u001b[39m=\u001b[39m embed(train_data)\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain data length: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(train_data))\n\u001b[1;32m     31\u001b[0m \u001b[39m# 14643\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m train_data \u001b[39m=\u001b[39m [tp\u001b[39m.\u001b[39mremove_punctuation(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m train_data]\n\u001b[1;32m     35\u001b[0m train_data \u001b[39m=\u001b[39m [sentence\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m train_data]\n\u001b[1;32m     36\u001b[0m train_data \u001b[39m=\u001b[39m [sentence\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m train_data]\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain data length: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(train_data))\n\u001b[1;32m     31\u001b[0m \u001b[39m# 14643\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m train_data \u001b[39m=\u001b[39m [tp\u001b[39m.\u001b[39mremove_punctuation(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m train_data]\n\u001b[1;32m     35\u001b[0m train_data \u001b[39m=\u001b[39m [sentence\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m train_data]\n\u001b[1;32m     36\u001b[0m train_data \u001b[39m=\u001b[39m [sentence\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m train_data]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tp' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = load_data()\n",
    "embeddings = embed(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_per_topic, top_n_words, topic_sizes = plot(embeddings, 100, 5, 0.0, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
