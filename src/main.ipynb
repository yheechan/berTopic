{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "from hdbscan.flat import (HDBSCAN_flat,\n",
    "\t\t\t\t\t\t  approximate_predict_flat,\n",
    "\t\t\t\t\t\t  membership_vector_flat,\n",
    "\t\t\t\t\t\t  all_points_membership_vectors_flat)\n",
    "# PLOT CLUSTERS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import text_process as tp\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, y):\n",
    "\n",
    "\ttexts = []\n",
    "\tlabels = []\n",
    "\n",
    "\twith open(path, encoding='utf-8') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\ttexts.append(line)\n",
    "\t\t\tlabels.append(y)\n",
    "\n",
    "\treturn texts, labels\n",
    "\n",
    "def load_data():\n",
    "\t# load data\n",
    "\tpaths = ['../data/train.negative.csv',\n",
    "\t\t\t\t'../data/train.non-negative.csv',\n",
    "\t\t\t\t'../data/test.negative.csv',\n",
    "\t\t\t\t'../data/test.non-negative.csv']\n",
    "\n",
    "\t# get data as list of dict with text and label\n",
    "\ttrain_neg_x_ls, train_neg_y_ls = get_data(paths[0], 1)\n",
    "\ttrain_non_x_ls, train_non_y_ls = get_data(paths[1], 0)\n",
    "\ttest_neg_x_ls, test_neg_y_ls = get_data(paths[2], 1)\n",
    "\ttest_non_x_ls, test_non_y_ls = get_data(paths[3], 0)\n",
    "\n",
    "\ttrain_data = train_neg_x_ls + train_non_x_ls\n",
    "\t# test_data = test_neg_x_ls + test_non_x_ls\n",
    "\n",
    "\tprint('train data length: ', len(train_data))\n",
    "\t# 14643\n",
    "\n",
    "\n",
    "\ttrain_data = [tp.remove_punctuation(sentence) for sentence in train_data]\n",
    "\ttrain_data = [sentence.lower() for sentence in train_data]\n",
    "\ttrain_data = [sentence.strip().split() for sentence in train_data]\n",
    "\n",
    "\ttrain_data = [sentence for sentence in train_data if len(sentence) >= 5]\n",
    "\n",
    "\ttrain_data = [' '.join(sentence) for sentence in train_data]\n",
    "\n",
    "\tnlp = spacy.load('en_core_web_sm')\n",
    "\ttrain_data = [[token.lemma_ for token in nlp(sentence)] for sentence in train_data]\n",
    "\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttrain_data = [ [word for word in sentence if not word in stop_words and word != ' '] for sentence in train_data]\n",
    "\n",
    "\ttrain_data = [' '.join(sentence) for sentence in train_data]\n",
    "\n",
    "\tprint('train data length (after preprocessing): ', len(train_data))\n",
    "\n",
    "\treturn train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(train_data):\n",
    "\t# BRING MODEL & ENCODE DATA to EMBEDDING VALUE\n",
    "\t'''\n",
    "\tdistilbert gives good balance between speed and performance\n",
    "\tsupports multi-lingual models\n",
    "\t'''\n",
    "\tfrom sentence_transformers import SentenceTransformer\n",
    "\tmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\tembeddings = model.encode(train_data, show_progress_bar=True)\n",
    "\n",
    "\tprint('embeddings: ', embeddings.shape)\n",
    "\t# (14643, 768)\n",
    "\n",
    "\treturn embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "\n",
    "\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names_out()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(embeddings, n_neighbors, n_components=5, min_dist=0.0, min_cluster_size=5, title=\"none\", min_samples=5):\n",
    "\t# REDUCE EMBEDDING DIMENSIONS using UMAP\n",
    "\tumap_embeddings = umap.UMAP(n_neighbors=n_neighbors,\n",
    "\t\t\t\t\t\t\t\tn_components=n_components,\n",
    "\t\t\t\t\t\t\t\tmin_dist=min_dist,\n",
    "\t\t\t\t\t\t\t\tmetric='cosine').fit_transform(embeddings)\n",
    "\tprint('\\ndone with reducing dimension')\n",
    "\n",
    "\n",
    "\t# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\t# CLUSTER EMBEDDINGS using HDBSCAN\n",
    "\t'''\n",
    "\tdoes not force data points meaning knows outliers\n",
    "\t'''\n",
    "\tcluster = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "\t\t\t\t\t\t\t  min_samples=min_samples,\n",
    "\t\t\t\t\t\t\t  cluster_selection_epsilon=0.0,\n",
    "\t\t\t\t\t\t\t  metric='euclidean',\n",
    "\t\t\t\t\t\t\t  cluster_selection_method='eom').fit(umap_embeddings)\n",
    "\t'''\n",
    "\tcluster = HDBSCAN_flat(umap_embeddings,\n",
    "\t\t\t\t\t\t\tcluster_selection_method='eom',\n",
    "\t\t\t\t\t\t\tn_clusters=20,\n",
    "\t\t\t\t\t\t\tmin_cluster_size=min_cluster_size)\n",
    "\t'''\t\n",
    "\n",
    "\tprint('\\ndone with clustering')\n",
    "\n",
    "\n",
    "\t# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\t# Prepare data\n",
    "\tumap_data = umap.UMAP(n_neighbors=n_neighbors,\n",
    "\t\t\t\t\t\t\tn_components=2,\n",
    "\t\t\t\t\t\t\tmin_dist=min_dist,\n",
    "\t\t\t\t\t\t\tmetric='cosine').fit_transform(embeddings)\n",
    "\tresult = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "\tresult['labels'] = cluster.labels_\n",
    "\n",
    "\n",
    "\t# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\t# Visualize clusters\n",
    "\tfig, ax = plt.subplots(figsize=(20, 10))\n",
    "\toutliers = result.loc[result.labels == -1, :]\n",
    "\tclustered = result.loc[result.labels != -1, :]\n",
    "\tplt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "\tplt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "\tplt.colorbar()\n",
    "\tplt.show()\n",
    "\n",
    "\tprint('\\ndone with plot')\n",
    "\n",
    "\n",
    "\t# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\t#c-TF-IDF\n",
    "\tdocs_df = pd.DataFrame(train_data, columns=[\"Doc\"])\n",
    "\tdocs_df['Topic'] = cluster.labels_\n",
    "\tdocs_df['Doc_ID'] = range(len(docs_df))\n",
    "\tdocs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
    "\n",
    "\n",
    "\t  \n",
    "\ttf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(train_data))\n",
    "\n",
    "\n",
    "\n",
    "\ttop_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "\ttopic_sizes = extract_topic_sizes(docs_df);\n",
    "\n",
    "\tprint(title)\n",
    "\tprint('n_neighbers: ' + str(n_neighbors))\n",
    "\tprint('n_components: ' + str(n_components))\n",
    "\tprint('min_dist: ' + str(min_dist))\n",
    "\tprint('min_cluster_size: ' + str(min_cluster_size))\n",
    "\tprint('min_samples: ' + str(min_samples))\n",
    "\n",
    "\tprint(topic_sizes.head(10))\n",
    "\tprint(len(topic_sizes))\n",
    "\n",
    "\n",
    "\tcnt = 0\n",
    "\tfor i, row in topic_sizes.iterrows():\n",
    "\t\ttopic_num = row['Topic']\n",
    "\t\ttopic_size = row['Size']\n",
    "\n",
    "\t\tif topic_num == -1: continue\n",
    "\n",
    "\t\tprint(topic_size, top_n_words[topic_num][:10])\n",
    "\n",
    "\t\tif cnt == 10: break\n",
    "\t\tcnt += 1\n",
    "\t\n",
    "\treturn docs_per_topic, top_n_words, topic_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(idx, topic_sizes, top_n_words, chng_neigh):\n",
    "    fn = chng_neigh[idx][4]\n",
    "\n",
    "    with open('../results/' + fn, 'w') as f:\n",
    "        f.write('n_neighbers: ' + str(chng_neigh[idx][0]) + '\\n')\n",
    "        f.write('n_components: ' + str(chng_neigh[idx][1]) + '\\n')\n",
    "        f.write('min_dist: ' + str(chng_neigh[idx][2]) + '\\n')\n",
    "        f.write('min_cluster_size: ' + str(chng_neigh[idx][3]) + '\\n\\n')\n",
    "        f.write('min_samples: ' + str(chng_neigh[idx][5]) + '\\n\\n')\n",
    "\n",
    "        total = str(topic_sizes['Size'].sum())\n",
    "        nonCluster = str(topic_sizes['Size'][0])\n",
    "        cluter_n = str(len(topic_sizes))\n",
    "\n",
    "        f.write('total number of cluster: ' + cluter_n + '\\n')\n",
    "        f.write('not in cluster: ' + nonCluster + '/' + total + '\\n\\n')\n",
    "        f.write('top 5 cluster')\n",
    "\n",
    "        cnt = 0\n",
    "        for i, row in topic_sizes.iterrows():\n",
    "            f.write('\\n')\n",
    "            topic_num = row['Topic']\n",
    "            topic_size = row['Size']\n",
    "\n",
    "            if topic_num == -1: continue\n",
    "\n",
    "            f.write('cluster #' + str(topic_num) + '\\n')\n",
    "            f.write('cluster size: ' + str(topic_size) + '\\n')\n",
    "\n",
    "            for word in top_n_words[topic_num][:10]:\n",
    "                f.write(str(word) + '\\n')\n",
    "            f.write('\\n')\n",
    "\n",
    "            if cnt == 10: break\n",
    "            cnt += 1\n",
    "\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data()\n",
    "embeddings = embed(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "# n_neighbors\n",
    "# n_components\n",
    "# min_dist\n",
    "# min_cluster\n",
    "# min_samples\n",
    "chng_neigh = [\n",
    "\t\t\t  # [5, 5, 0.0, 15, 'test01-1'], # o 6302 - 127\n",
    "\t\t\t  # [5, 64, 0.0, 15, 'test02-1'], # o 6394 - 114\n",
    "\t\t\t  # [3, 90, 0.0, 15, 'test03-1'], # o 4971 - 164\n",
    "\t\t\t  # [3, 64, 0.0, 15, 'test04-1'], # o 4930 - 174\n",
    "\t\t\t  [3, 90, 0.0, 15, 'test-a01-1', 15],\n",
    "\t\t\t  [7, 90, 0.0, 15, 'test-a01-1', 15],\n",
    "\t\t\t  [3, 32, 0.0, 15, 'test-a01-2', 10],\n",
    "\t\t\t  [7, 32, 0.0, 15, 'test-a01-2', 10],\n",
    "\t\t\t  # [7, 64, 0.0, 60, 'test-a01-2', 60],\n",
    "\t\t\t  # [5, 64, 0.0, 60, 'test-a01-3', 2],\n",
    "\t\t\t  # [5, 64, 0.0, 60, 'test-a01-4', 2],\n",
    "\t\t\t  # [7, 128, 0.0, 60, 'test-a01-4', 10],\n",
    "\t\t\t]\n",
    "\n",
    "\n",
    "for i in range(len(chng_neigh)):\n",
    "\tdocs_per_topic, top_n_words, topic_sizes = plot(embeddings, chng_neigh[i][0], chng_neigh[i][1], chng_neigh[i][2], chng_neigh[i][3], chng_neigh[i][4], chng_neigh[i][5])\n",
    "\twrite_results(i, topic_sizes, top_n_words, chng_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "docs_per_topic, top_n_words, topic_sizes = plot(embeddings, 100, 5, 0.0, 15)\n",
    "fn = 'test1'\n",
    "\n",
    "write_results(0, topic_sizes, top_n_words, [[100, 5, 0.0, 15, 'test1']])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "for i in range(20):\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(tf_idf.T)\n",
    "    np.fill_diagonal(similarities, 0)\n",
    "\n",
    "    # Extract label to merge into and from where\n",
    "    topic_sizes = docs_df.groupby(['Topic']).count().sort_values(\"Doc\", ascending=False).reset_index()\n",
    "    topic_to_merge = topic_sizes.iloc[-1].Topic\n",
    "    topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
    "\n",
    "    # Adjust topics\n",
    "    docs_df.loc[docs_df.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
    "    old_topics = docs_df.sort_values(\"Topic\").Topic.unique()\n",
    "    map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)}\n",
    "    docs_df.Topic = docs_df.Topic.map(map_topics)\n",
    "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
    "\n",
    "    # Calculate new topic words\n",
    "    m = len(data)\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m)\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "348b9cd948ce87438be2e622031b2ecfa29bc2d3ecc0fd03127b9a24b30227df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
